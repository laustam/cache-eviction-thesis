\chapter{Introduction}\label{chapter:introduction}

\outline{What is the cache and why is it important}

Since the early 2000s, the process of information retrieval has undergone a fundamental change. Where individuals once relied on libraries, printed newspapers, or other physical media to access information, the advent and widespread adoption of the internet have shifted this process to the digital realm. The deep integration of the internet into everyday life has made information retrieval faster, more convenient, and more central than ever before.

This shift has resulted in a rapidly increasing volume of web traffic ("WWW traffic"), thus placing enormous pressure on web systems to deliver content quickly and reliably \cite{web-cache-overview}. In fact, recent market research suggests that fixed web traffic will more than double by 2030 \cite{digital-global-report}. One critical technique used to meet these demands is to reduce network latency through \keyword{web caching} - the temporary storage of frequently accessed data closer to users or in faster-access locations, which reduces server load, minimizes bandwidth consumption, and improves the perceived latency by the end user \cite{latency-caching, web-cache-overview}. 

\outline{How does the cache decide what to evict $\Rightarrow$ introduce cache eviction policies}

Cache systems are designed with speed of delivery at their core; however, their storage capacity is limited. To manage this tradeoff between speed and space constraints effectively, caches implement a strategy to manage stored items, effectively deciding which items to "throw out" or evict when storage is full. These strategies are known as \keyword{cache eviction algorithms} or \keyword{cache replacement policies} \cite{web-cache-overview}.


\outline{Different types of cache eviction algorithms. Not all are used in practice}

Choosing an appropriate cache eviction algorithm for a given system requires an understanding that there is no universal "one size fits all" solution. Numerous algorithms exist, each designed to exploit specific patterns commonly observed in real-world scenarios. For instance, web systems often follow a \keyword{Zipf}-like distribution, where a small number of items account for a large portion of requests \cite{sieve, web-cache-overview}. This skewed access pattern is distinct from the more uniform patterns observed in traditional memory systems, thereby requiring specialized eviction strategies tailored to web workloads \cite{web-cache-overview}.


\section{Problem Statement}

\outline{Something about increased complexity of newer algorithms compared to older ones?}

The continuous increase in web traffic \cite{digital-global-report} necessitates further exploration of cache eviction algorithms to optimize web cache utilization. Concurrently, researchers have observed that the newer cache eviction algorithms are too complex \cite{sieve, s3-fifo}. \citeauthor{sieve} \cite{sieve} highlight the importance of simplicity as a "key appealing feature for an algorithm to be deployed in production since it commonly correlates with effectiveness, maintainability, scalability, and low overhead" \cite{sieve}. The authors found that increased algorithm complexity introduces a range of issues, including debugging difficulties, decreased efficiency, reduced throughput, scalability challenges due to increased per-object metadata requirements, and complexities in parameter tuning for newer machine learning-based algorithms.

In addition to highlighting the increased complexity of recently developed algorithms, \citeauthor{sieve} introduce SIEVE, a novel cache eviction algorithm they claim "achieves both simplicity and efficiency" \cite{sieve}. More than just a typical cache eviction algorithm, SIEVE functions as a \keyword{turn-key cache primitive}: a basic, pre-built, and easy-to-use caching mechanism designed for minimal configuration and seamless integration into larger caching systems. Other cache-primitives that are widely used in today's cache systems include the well-known LRU and FIFO algorithms \cite{sieve, s3-fifo}.


\section{Research Goal}\label{sec: research_q}

\outline{Cache primitives, SIEVE vs LRU and FIFO, miss ratio}

This research primarily aims to examine the effectiveness of \citeauthor{sieve}'s \cite{sieve} recent SIEVE turn-key cache primitive on workloads exhibiting Zipf-like distributions. We will compare its performance with that of the older cache primitives, LRU and FIFO. Additionally, we further assess the effectiveness of the different primitives across different parameters, such as the Zipfian skewness parameter $\alpha$ and the relative cache size. Ultimately, this work aims to determine whether SIEVE is sufficiently competitive to be considered a viable or even preferred alternative to LRU and FIFO in production systems exhibiting Zipf-like access patterns. Performance is evaluated using the miss ratio as the comparative metric, and web traces used in this work are generated synthetically by sampling from Zipfian distributions.


% How do SIEVE compare to existing cache primitives? Is it competitive enough to be considered as a real competitor to widely used cache primitives LRU and FIFO?

% How is the miss ratio of the different cache primitives?

% And how does cache size affect the effectiveness of different cache primitives?

\section{Thesis Outline}

This section provides a roadmap for the contents of this work. The first chapter, \nameref{chapter:introduction}, introduces the research topic, defines the problem statement, and outlines the research goals. Following this, \nameref{chapter:background} establishes the theoretical foundation by reviewing key concepts, including web caching, cache eviction primitives, and Zipfian distributions. The methodology is then detailed across two chapters: \nameref{chapter:design}, which describes the implementation of the cache simulator, and \nameref{chapter:experimental-setup}, which outlines the specific parameters and process used for the simulations. The findings of these experiments are presented in \nameref{chapter:results}, followed by a critical interpretation in \nameref{chapter:discussion}. Finally, \nameref{chapter:conclusion} summarizes the main contributions of the work and suggests some directions for future research.